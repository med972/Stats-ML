{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "import numpy as np\n",
    "import re\n",
    "from sentimentanalysis import *\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n",
      "2000 documents\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading dataset\")\n",
    "\n",
    "from glob import glob\n",
    "filenames_neg = sorted(glob(op.join('/Users/mehdiregina/Documents/jupyter/MDI343/TPTextMining', 'data', 'imdb1', 'neg', '*.txt')))\n",
    "filenames_pos = sorted(glob(op.join('/Users/mehdiregina/Documents/jupyter/MDI343/TPTextMining', 'data', 'imdb1', 'pos', '*.txt')))\n",
    "\n",
    "texts_neg = [open(f).read() for f in filenames_neg] #liste de textes negatifs\n",
    "texts_pos = [open(f).read() for f in filenames_pos] #liste de textes positifs\n",
    "texts = texts_neg + texts_pos #liste de tous les textes\n",
    "y = np.ones(len(texts), dtype=np.int)\n",
    "y[:len(texts_neg)] = 0. #output 0 pour les negatifs, 1 pour les positifs\n",
    "\n",
    "print(\"%d documents\" % len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation du classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Compléter la fonction count_words qui va compter le nombre d’occurrences de chaque mot dans une liste de string et renvoyer le vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_words(texts):\n",
    "    \"\"\"Vectorize text : return count of each word in the text snippets\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : list of str\n",
    "        The texts corpus\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    vocabulary : dict\n",
    "        A dictionary that points to an index in counts for each word.\n",
    "    counts : ndarray, shape (n_samples, n_features)\n",
    "        The counts of each word in each text.\n",
    "        n_samples == number of documents.\n",
    "        n_features == number of words in vocabulary.\n",
    "    \"\"\"\n",
    "    words = set()\n",
    "    vocabulary = dict()\n",
    "    \n",
    "    #1e etape get text vocabulary\n",
    "    for text in texts :\n",
    "        for word in re.split(r'[\\n\\s\\.,-;\\':]+',text.strip().lower()):\n",
    "            words.add(word)\n",
    "    \n",
    "    #build a dict word-index : no index in set !\n",
    "    vocabulary = dict(zip(words, range(0,len(words))))\n",
    "    \n",
    "    #build count matrix\n",
    "    count = np.zeros((len(texts),len(words)))\n",
    "    for i,text in enumerate(texts):\n",
    "        for word in re.split(r'[\\n\\s\\.,-;\\':]+',text.strip().lower()):\n",
    "            j = vocabulary[word]\n",
    "            count[i,j]+=1\n",
    "    \n",
    "    return vocabulary,count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voc, matrix = count_words(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0,\n",
       " 'lavish': 1,\n",
       " 'contraceptive': 2,\n",
       " 'laughter': 3,\n",
       " 'incandescent': 4,\n",
       " 'near': 5,\n",
       " 'helgenberger': 6,\n",
       " 'juxtaposition': 7,\n",
       " 'puppeteering': 8,\n",
       " 'dermot': 9,\n",
       " 'communicative': 10,\n",
       " 'fuentes': 11,\n",
       " 'sickened': 12,\n",
       " 'kubrick': 13,\n",
       " 'engenders': 14,\n",
       " 'protestations': 15,\n",
       " 'heisted': 16,\n",
       " 'whose': 17,\n",
       " 'tricks': 18,\n",
       " 'lunkheads': 19,\n",
       " 'estella': 20,\n",
       " 'wage': 21,\n",
       " 'innocent': 22,\n",
       " 'grueling': 23,\n",
       " 'capital': 24,\n",
       " 'transfer': 25,\n",
       " 'flourishes': 26,\n",
       " 'hugged': 27,\n",
       " 'display': 28,\n",
       " 'uneventful': 29,\n",
       " 'lauper': 30,\n",
       " 'spottiswoode': 31,\n",
       " 'looming': 32,\n",
       " 'ginty': 33,\n",
       " 'intent': 34,\n",
       " 'balloon': 35,\n",
       " 'irresistable': 36,\n",
       " 'prospected': 37,\n",
       " 'fine': 38,\n",
       " 'pheiffer': 39,\n",
       " 'integration': 40,\n",
       " 'deck': 41,\n",
       " 'mourn': 42,\n",
       " 'mushroom': 43,\n",
       " 'superb': 44,\n",
       " 'unremittingly': 45,\n",
       " 'tonino': 46,\n",
       " 'skins': 47,\n",
       " 'carriage': 48,\n",
       " 'craftsmen': 49,\n",
       " 'mu': 50,\n",
       " 'searles': 51,\n",
       " 'hawkins': 52,\n",
       " 'bite': 53,\n",
       " 'valiantly': 54,\n",
       " 'terl': 55,\n",
       " 'easter': 56,\n",
       " 'prosperous': 57,\n",
       " 'kureishi': 58,\n",
       " 'naive': 59,\n",
       " 'deconstructs': 60,\n",
       " 'cloth': 61,\n",
       " 'lest': 62,\n",
       " 'vh': 63,\n",
       " 'competitive': 64,\n",
       " 'merge': 65,\n",
       " 'smorgasbord': 66,\n",
       " 'crichton': 67,\n",
       " 'broods': 68,\n",
       " 'internally': 69,\n",
       " 'forties': 70,\n",
       " 'legitimately': 71,\n",
       " 'strathairn': 72,\n",
       " 'consistent': 73,\n",
       " 'notions': 74,\n",
       " 'mismatched': 75,\n",
       " 'drinker': 76,\n",
       " 'pickin': 77,\n",
       " 'filimg': 78,\n",
       " 'descended': 79,\n",
       " 'bug': 80,\n",
       " 'rite': 81,\n",
       " 'fundamentally': 82,\n",
       " 'nygard': 83,\n",
       " 'systems': 84,\n",
       " 'immolation': 85,\n",
       " 'merchandising': 86,\n",
       " 'polynesian': 87,\n",
       " 'samantha': 88,\n",
       " 'creams': 89,\n",
       " 'opened': 90,\n",
       " 'glimpsed': 91,\n",
       " 'leland': 92,\n",
       " 'portait': 93,\n",
       " 'peerless': 94,\n",
       " 'reptiles': 95,\n",
       " 'rambunctous': 96,\n",
       " 'oriential': 97,\n",
       " 'sagan': 98,\n",
       " 'incorrect': 99,\n",
       " 'wasp': 100,\n",
       " 'micro': 101,\n",
       " 'snatchers': 102,\n",
       " 'divert': 103,\n",
       " 'bereaved': 104,\n",
       " 'weismuller': 105,\n",
       " 'berlin': 106,\n",
       " '[katarina]': 107,\n",
       " 'rudiments': 108,\n",
       " 'tad': 109,\n",
       " 'giggle': 110,\n",
       " 'bashes': 111,\n",
       " 'anytown': 112,\n",
       " 'focal': 113,\n",
       " 'din': 114,\n",
       " 'ecological': 115,\n",
       " 'loyalties': 116,\n",
       " 'crocheting': 117,\n",
       " 'mainframe': 118,\n",
       " 'becomming': 119,\n",
       " 'jen': 120,\n",
       " 'coworker': 121,\n",
       " 'satisfying': 122,\n",
       " 'grand': 123,\n",
       " 'trilian': 124,\n",
       " 'blume': 125,\n",
       " 'coterie': 126,\n",
       " 'fuel': 127,\n",
       " 'juries': 128,\n",
       " 'overpowers': 129,\n",
       " 'journeyed': 130,\n",
       " 'embarass': 131,\n",
       " 'thematic': 132,\n",
       " 'fan': 133,\n",
       " 'diluting': 134,\n",
       " 'priveleged': 135,\n",
       " 'fealty': 136,\n",
       " 'treats': 137,\n",
       " 'africans': 138,\n",
       " '_halloween': 139,\n",
       " 'wickedness': 140,\n",
       " 'feuds': 141,\n",
       " 'brosnon': 142,\n",
       " 'shpadoinkle': 143,\n",
       " 'illegally': 144,\n",
       " 'otherness': 145,\n",
       " 'serviceman': 146,\n",
       " 'portuguese': 147,\n",
       " 'undercuts': 148,\n",
       " 'fornication': 149,\n",
       " 'diaz': 150,\n",
       " 'clip': 151,\n",
       " 'entertains': 152,\n",
       " 'retardant': 153,\n",
       " 'reprising': 154,\n",
       " 'possessed': 155,\n",
       " 'smooths': 156,\n",
       " 'slums': 157,\n",
       " 'inquired': 158,\n",
       " 'masur': 159,\n",
       " 'dillon': 160,\n",
       " 'skimp': 161,\n",
       " 'natch': 162,\n",
       " 'brothel': 163,\n",
       " 'glazed': 164,\n",
       " 'billing': 165,\n",
       " 'ignoramus': 166,\n",
       " 'clang': 167,\n",
       " 'avoiding': 168,\n",
       " 'usurp': 169,\n",
       " 'iffy': 170,\n",
       " 'circuitry': 171,\n",
       " 'rosenberg': 172,\n",
       " 'remedy': 173,\n",
       " 'poisoning': 174,\n",
       " 'segues': 175,\n",
       " 'modestly': 176,\n",
       " 'proctor': 177,\n",
       " 'onanism': 178,\n",
       " 'aaaaaaaaah': 179,\n",
       " 'khanh': 180,\n",
       " 'bitterness': 181,\n",
       " 'innuendoes': 182,\n",
       " 'captor': 183,\n",
       " 'nears': 184,\n",
       " 'rise': 185,\n",
       " 'cantrell': 186,\n",
       " 'giulianni': 187,\n",
       " 'cells': 188,\n",
       " 'thankfully': 189,\n",
       " 'bizarre': 190,\n",
       " 'loafer': 191,\n",
       " 'afford': 192,\n",
       " 'endemic': 193,\n",
       " 'internalized': 194,\n",
       " 'penciller': 195,\n",
       " 'opposite': 196,\n",
       " 'reanimator': 197,\n",
       " 'tiranny': 198,\n",
       " 'blond': 199,\n",
       " 'oppposed': 200,\n",
       " 'mugs': 201,\n",
       " 'suspects': 202,\n",
       " 'ripping': 203,\n",
       " 'gumshoe': 204,\n",
       " 'elegant': 205,\n",
       " 'sabrina': 206,\n",
       " 'adhesive': 207,\n",
       " 'exacts': 208,\n",
       " 'undergone': 209,\n",
       " 'coutroom': 210,\n",
       " 'nelken': 211,\n",
       " 'guaspari': 212,\n",
       " 'eunuch': 213,\n",
       " 'caterpillar': 214,\n",
       " 'mullan': 215,\n",
       " 'exonerated': 216,\n",
       " 'zycie': 217,\n",
       " 'lock': 218,\n",
       " 'football': 219,\n",
       " 'gio': 220,\n",
       " 'convenience': 221,\n",
       " 'intricacies': 222,\n",
       " 'doubtlessly': 223,\n",
       " 'potential': 224,\n",
       " 'pipe': 225,\n",
       " 'factored': 226,\n",
       " 'snorts': 227,\n",
       " 'fresh': 228,\n",
       " '_the_fugitive_': 229,\n",
       " 'escorted': 230,\n",
       " 'snoozer': 231,\n",
       " 'guelph': 232,\n",
       " 'doreen': 233,\n",
       " 'certifiably': 234,\n",
       " 'dares': 235,\n",
       " 'respite': 236,\n",
       " 'playset': 237,\n",
       " 'britain': 238,\n",
       " 'messing': 239,\n",
       " 'exterminating': 240,\n",
       " 'logy': 241,\n",
       " 'afoot': 242,\n",
       " 'ferrets': 243,\n",
       " 'superimposed': 244,\n",
       " 'saddening': 245,\n",
       " 'endorsement': 246,\n",
       " 'judgments': 247,\n",
       " 'rays': 248,\n",
       " 'saturated': 249,\n",
       " 'johanssen': 250,\n",
       " 'ingesting': 251,\n",
       " 'collapse': 252,\n",
       " 'yulin': 253,\n",
       " 'graphical': 254,\n",
       " 'gestate': 255,\n",
       " 'confusingly': 256,\n",
       " 'deftness': 257,\n",
       " 'deja': 258,\n",
       " 'acquaintance': 259,\n",
       " 'fyfe': 260,\n",
       " 'pear': 261,\n",
       " 'alfie': 262,\n",
       " 'wetmore': 263,\n",
       " 'mediator': 264,\n",
       " 'lurk': 265,\n",
       " 'controlled': 266,\n",
       " 'menial': 267,\n",
       " 'dismissive': 268,\n",
       " 'videotape': 269,\n",
       " 'therapist': 270,\n",
       " 'bethany': 271,\n",
       " 'rituals': 272,\n",
       " 'nihilist': 273,\n",
       " 'upwardly': 274,\n",
       " 'impulsively': 275,\n",
       " 'debonair': 276,\n",
       " 'booked': 277,\n",
       " 'flaws': 278,\n",
       " 'mcferran': 279,\n",
       " 'consoling': 280,\n",
       " 'launched': 281,\n",
       " 'ambush': 282,\n",
       " 'grudgingly': 283,\n",
       " 'snubbed': 284,\n",
       " 'cinemax': 285,\n",
       " 'junkies': 286,\n",
       " 'jammed': 287,\n",
       " 'arte': 288,\n",
       " 'thorny': 289,\n",
       " 'laugh': 290,\n",
       " 'woolly': 291,\n",
       " 'hangers': 292,\n",
       " 'explanation': 293,\n",
       " 'tugboat': 294,\n",
       " 'takaaki': 295,\n",
       " 'workplace': 296,\n",
       " 'heil': 297,\n",
       " 'sooner': 298,\n",
       " 'gruelling': 299,\n",
       " 'uneasily': 300,\n",
       " 'ninety': 301,\n",
       " 'payoff': 302,\n",
       " 'describing': 303,\n",
       " 'subdued': 304,\n",
       " 'reb': 305,\n",
       " '`pulp': 306,\n",
       " 'oversight': 307,\n",
       " 'lebanon': 308,\n",
       " 'attached': 309,\n",
       " 'tappan': 310,\n",
       " 'passed': 311,\n",
       " 'excising': 312,\n",
       " 'pleasured': 313,\n",
       " 'veneer': 314,\n",
       " 'summarize': 315,\n",
       " 'muster': 316,\n",
       " 'tenny': 317,\n",
       " 'amour': 318,\n",
       " 'shirtless': 319,\n",
       " 'sprucing': 320,\n",
       " 'tremors': 321,\n",
       " 'sadists': 322,\n",
       " 'unabomber': 323,\n",
       " 'glumly': 324,\n",
       " 'blusterous': 325,\n",
       " 'costars': 326,\n",
       " 'pained': 327,\n",
       " 'ejogo': 328,\n",
       " 'transpose': 329,\n",
       " 'foreshadow': 330,\n",
       " 'impatient': 331,\n",
       " 'emergence': 332,\n",
       " 'success': 333,\n",
       " 'flippant': 334,\n",
       " 'frustrates': 335,\n",
       " 'sheltered': 336,\n",
       " 'summit': 337,\n",
       " 'obesity': 338,\n",
       " 'contacts': 339,\n",
       " 'hime': 340,\n",
       " 'pigeonholed': 341,\n",
       " 'violates': 342,\n",
       " 'regaled': 343,\n",
       " 'squirrels': 344,\n",
       " 'disruption': 345,\n",
       " 'scaffolding': 346,\n",
       " 'schmaltzy': 347,\n",
       " 'spitting': 348,\n",
       " 'shue': 349,\n",
       " 'moguls': 350,\n",
       " 'obliges': 351,\n",
       " 'downward': 352,\n",
       " 'staunch': 353,\n",
       " 'middleton': 354,\n",
       " 'clicking': 355,\n",
       " 'heartbreaks': 356,\n",
       " 'comfortable': 357,\n",
       " 'lovely': 358,\n",
       " 'predictable': 359,\n",
       " '_his_': 360,\n",
       " 'stress': 361,\n",
       " 'brio': 362,\n",
       " 'pinpoint': 363,\n",
       " 'rep': 364,\n",
       " 'soderburgh': 365,\n",
       " 'eschewing': 366,\n",
       " 'dismay': 367,\n",
       " 'motiveless': 368,\n",
       " '_animal': 369,\n",
       " 'comepete': 370,\n",
       " 'anarchists': 371,\n",
       " 'reset': 372,\n",
       " 'jacobim': 373,\n",
       " 'attention': 374,\n",
       " 'unfilmable': 375,\n",
       " 'gaetan': 376,\n",
       " 'snapshot': 377,\n",
       " 'damnedest': 378,\n",
       " 'murderer': 379,\n",
       " 'nicola': 380,\n",
       " 'stoically': 381,\n",
       " 'fairfolk': 382,\n",
       " 'theatres': 383,\n",
       " 'scientologists': 384,\n",
       " 'loophole': 385,\n",
       " 'horrid': 386,\n",
       " 'typical': 387,\n",
       " 'heart_': 388,\n",
       " 'triggering': 389,\n",
       " 'starlet': 390,\n",
       " 'vinegar': 391,\n",
       " 'faultless': 392,\n",
       " 'reunites': 393,\n",
       " 'revoltingly': 394,\n",
       " 'shrieking': 395,\n",
       " 'aide': 396,\n",
       " 'quirks': 397,\n",
       " 'regions': 398,\n",
       " 'caption': 399,\n",
       " 'contest': 400,\n",
       " 'stuffing': 401,\n",
       " 'permit': 402,\n",
       " 'crafty': 403,\n",
       " 'rag': 404,\n",
       " 'quotas': 405,\n",
       " 'penultimate': 406,\n",
       " 'joss': 407,\n",
       " 'flanked': 408,\n",
       " 'expressing': 409,\n",
       " 'wehrmacht': 410,\n",
       " 'noisy': 411,\n",
       " 'hitch': 412,\n",
       " 'erickson': 413,\n",
       " 'prwhen': 414,\n",
       " 'unfeasible': 415,\n",
       " 'arching': 416,\n",
       " 'volunteers': 417,\n",
       " 'contributors': 418,\n",
       " 'marlowe': 419,\n",
       " 'elegance': 420,\n",
       " 'schwartzeneggar': 421,\n",
       " 'assuredness': 422,\n",
       " 'gawky': 423,\n",
       " 'santiago': 424,\n",
       " 'palestinian': 425,\n",
       " 'dour': 426,\n",
       " 'sovereign': 427,\n",
       " 'sawa': 428,\n",
       " 'rennie': 429,\n",
       " 'depressant': 430,\n",
       " 'static': 431,\n",
       " 'wad': 432,\n",
       " 'concoction': 433,\n",
       " 'unholy': 434,\n",
       " 'laurence': 435,\n",
       " 'bottom': 436,\n",
       " 'demonstration': 437,\n",
       " 'sanctioned': 438,\n",
       " 'trendsetters': 439,\n",
       " 'insecurities': 440,\n",
       " 'woodies': 441,\n",
       " 'hockley': 442,\n",
       " 'lianna': 443,\n",
       " 'transfers': 444,\n",
       " 'unconvincing': 445,\n",
       " 'outline': 446,\n",
       " 'heartstopping': 447,\n",
       " 'work': 448,\n",
       " 'haysbert': 449,\n",
       " 'pennebaker': 450,\n",
       " 'flik': 451,\n",
       " 'razzie': 452,\n",
       " 'fistfights': 453,\n",
       " 'wanderer': 454,\n",
       " 'ostott': 455,\n",
       " 'twitches': 456,\n",
       " 'missionary': 457,\n",
       " 'walter': 458,\n",
       " 'lan': 459,\n",
       " 'fens': 460,\n",
       " 'evaluating': 461,\n",
       " 'accolades': 462,\n",
       " 'melding': 463,\n",
       " 'hexagons': 464,\n",
       " 'umptenth': 465,\n",
       " 'wreaked': 466,\n",
       " 'venus': 467,\n",
       " 'yanne': 468,\n",
       " 'dwellers': 469,\n",
       " 'subs': 470,\n",
       " 'warmest': 471,\n",
       " 'rubick': 472,\n",
       " 'cinematic': 473,\n",
       " 'brennick': 474,\n",
       " 'stigmata': 475,\n",
       " 'tbwp': 476,\n",
       " 'ellroy': 477,\n",
       " 'plimpton': 478,\n",
       " 'preschool': 479,\n",
       " 'icon': 480,\n",
       " 'calamitous': 481,\n",
       " 'stagnated': 482,\n",
       " 'policemen': 483,\n",
       " 'enrich': 484,\n",
       " 'bilking': 485,\n",
       " 'maud': 486,\n",
       " 'thuds': 487,\n",
       " 'tortured': 488,\n",
       " 'reworked': 489,\n",
       " 'underline': 490,\n",
       " 'wheels': 491,\n",
       " 'lange': 492,\n",
       " 'possession': 493,\n",
       " 'foreknowledge': 494,\n",
       " 'barbell': 495,\n",
       " 'habanera': 496,\n",
       " 'tidied': 497,\n",
       " 'technologies': 498,\n",
       " 'young': 499,\n",
       " 'disapproving': 500,\n",
       " 'naidu': 501,\n",
       " 'accompanies': 502,\n",
       " 'offered': 503,\n",
       " 'egon': 504,\n",
       " 'chracters': 505,\n",
       " 'ironic': 506,\n",
       " 'twosomes': 507,\n",
       " 'virgil': 508,\n",
       " 'rightfully': 509,\n",
       " 'corpses': 510,\n",
       " 'wises': 511,\n",
       " 'taran': 512,\n",
       " 'venkman': 513,\n",
       " 'kewpies': 514,\n",
       " 'abject': 515,\n",
       " 'prefered': 516,\n",
       " 'twenties': 517,\n",
       " 'workaholics': 518,\n",
       " 'mein': 519,\n",
       " 'mushrooms': 520,\n",
       " 'push': 521,\n",
       " 'sailors': 522,\n",
       " 'realising': 523,\n",
       " 'dissembles': 524,\n",
       " 'dexterity': 525,\n",
       " 'dogstar': 526,\n",
       " 'copywriters': 527,\n",
       " 'billowing': 528,\n",
       " 'equitably': 529,\n",
       " 'inroad': 530,\n",
       " 'snorkle': 531,\n",
       " 'lifeboats': 532,\n",
       " 'jax': 533,\n",
       " 'landsbury': 534,\n",
       " 'ode': 535,\n",
       " 'educating': 536,\n",
       " 'hunger': 537,\n",
       " 'burton': 538,\n",
       " 'lollipop': 539,\n",
       " 'ey': 540,\n",
       " 'with]': 541,\n",
       " 'betty': 542,\n",
       " 'manhunter': 543,\n",
       " 'sarandon': 544,\n",
       " 'puffy': 545,\n",
       " 'satyrical': 546,\n",
       " 'hypocrite': 547,\n",
       " 'oedipal': 548,\n",
       " 'delacroix': 549,\n",
       " 'urgayle': 550,\n",
       " 'fleischer': 551,\n",
       " 'counting': 552,\n",
       " 'scarlett': 553,\n",
       " 'roan': 554,\n",
       " 'gaze': 555,\n",
       " 'irvin': 556,\n",
       " 'negates': 557,\n",
       " 'formulates': 558,\n",
       " 'yao': 559,\n",
       " 'sherriff': 560,\n",
       " 'troop': 561,\n",
       " 'workplaces': 562,\n",
       " 'electrocution': 563,\n",
       " 'heartedfrom': 564,\n",
       " 'webb': 565,\n",
       " 'imprint': 566,\n",
       " 'uplifiting': 567,\n",
       " 'zallian': 568,\n",
       " 'listener': 569,\n",
       " 'visiting': 570,\n",
       " 'including': 571,\n",
       " 'interments': 572,\n",
       " 'grunting': 573,\n",
       " 'strip': 574,\n",
       " 'collectively': 575,\n",
       " 'sector': 576,\n",
       " 'delirious': 577,\n",
       " 'fronted': 578,\n",
       " 'nixon': 579,\n",
       " 'proper': 580,\n",
       " 'falter': 581,\n",
       " 'senge': 582,\n",
       " 'idaho': 583,\n",
       " 'hatcher': 584,\n",
       " 'benj': 585,\n",
       " 'effortlessly': 586,\n",
       " 'dodie': 587,\n",
       " 'synergy': 588,\n",
       " 'doubles': 589,\n",
       " 'examples': 590,\n",
       " 'scratcher': 591,\n",
       " 'westerns': 592,\n",
       " 'fraker': 593,\n",
       " 'tutelage': 594,\n",
       " 'franz': 595,\n",
       " 'scans': 596,\n",
       " 'wabbit': 597,\n",
       " 'aussies': 598,\n",
       " 'understate': 599,\n",
       " 'gaming': 600,\n",
       " 'limo': 601,\n",
       " 'rehearsed': 602,\n",
       " 'softens': 603,\n",
       " 'venality': 604,\n",
       " 'cereal': 605,\n",
       " 'cutter': 606,\n",
       " 'advertisement': 607,\n",
       " 'militant': 608,\n",
       " 'alcoholics': 609,\n",
       " 'upsetting': 610,\n",
       " 'fastened': 611,\n",
       " 'cheapjack': 612,\n",
       " 'eriq': 613,\n",
       " 'oily': 614,\n",
       " 'admittingly': 615,\n",
       " 'redeem': 616,\n",
       " 'precedes': 617,\n",
       " 'flirtatious': 618,\n",
       " 'savior': 619,\n",
       " 'andrus': 620,\n",
       " 'contributing': 621,\n",
       " 'hyperkinetic': 622,\n",
       " 'fed': 623,\n",
       " 'jersey': 624,\n",
       " 'sec': 625,\n",
       " 'hijinks': 626,\n",
       " 'tristine': 627,\n",
       " '_patlabor': 628,\n",
       " 'landscaper': 629,\n",
       " 'commend': 630,\n",
       " 'etre': 631,\n",
       " 'kissed_': 632,\n",
       " 'uli': 633,\n",
       " 'prose': 634,\n",
       " 'bland': 635,\n",
       " 'tiredness': 636,\n",
       " 'traditional': 637,\n",
       " 'chained': 638,\n",
       " 'sparticus': 639,\n",
       " 'yessuh': 640,\n",
       " 'cricket': 641,\n",
       " 'elements': 642,\n",
       " 'entrail': 643,\n",
       " 'ingest': 644,\n",
       " 'shells': 645,\n",
       " 'ruminations': 646,\n",
       " 'sculptor': 647,\n",
       " 'surprising': 648,\n",
       " 'onatopp': 649,\n",
       " 'failed': 650,\n",
       " 'oxygen': 651,\n",
       " 'apartheid': 652,\n",
       " 'bloated': 653,\n",
       " 'psychic': 654,\n",
       " 'lender': 655,\n",
       " 'proceedings': 656,\n",
       " 'symbolise': 657,\n",
       " 'pendel': 658,\n",
       " 'serum': 659,\n",
       " 'ferrini': 660,\n",
       " 'locales': 661,\n",
       " 'ho': 662,\n",
       " 'digitalized': 663,\n",
       " 'plagiarize': 664,\n",
       " 'keels': 665,\n",
       " 'violinist': 666,\n",
       " 'raceway': 667,\n",
       " 'stupefying': 668,\n",
       " 'incompletely': 669,\n",
       " 'pastiches': 670,\n",
       " 'ensue': 671,\n",
       " 'mst': 672,\n",
       " 'deliciously': 673,\n",
       " 'effaced': 674,\n",
       " 'della': 675,\n",
       " 'denounces': 676,\n",
       " 'discouraging': 677,\n",
       " 'profess': 678,\n",
       " 'coordinators': 679,\n",
       " 'fresa': 680,\n",
       " 'glades': 681,\n",
       " 'berliner': 682,\n",
       " 'warmth': 683,\n",
       " 'delapidated': 684,\n",
       " 'masturbation': 685,\n",
       " 'communication': 686,\n",
       " 'incisive': 687,\n",
       " 'conciliatory': 688,\n",
       " 'fooled': 689,\n",
       " 'sincere': 690,\n",
       " 'fuckin': 691,\n",
       " 'acclaimed': 692,\n",
       " 'tote': 693,\n",
       " 'herrier': 694,\n",
       " 'commence': 695,\n",
       " 'unexploited': 696,\n",
       " 'travelled': 697,\n",
       " 'dizzyingly': 698,\n",
       " 'unimpressive': 699,\n",
       " 'transistor': 700,\n",
       " 'agnostic': 701,\n",
       " 'continuous': 702,\n",
       " 'drape': 703,\n",
       " 'inquiries': 704,\n",
       " 'massacering': 705,\n",
       " 'likability': 706,\n",
       " 'maltese': 707,\n",
       " 'overabundance': 708,\n",
       " 'interviewing': 709,\n",
       " 'implying': 710,\n",
       " 'raindrops': 711,\n",
       " 'sometimes': 712,\n",
       " 'idelogy': 713,\n",
       " 'emmannuelle': 714,\n",
       " 'dislikable': 715,\n",
       " 'dreyfus': 716,\n",
       " 'volleyball': 717,\n",
       " 'cerrano': 718,\n",
       " 'wiper': 719,\n",
       " 'shining': 720,\n",
       " 'raiders': 721,\n",
       " 'neighboring': 722,\n",
       " 'howie': 723,\n",
       " 'clifford': 724,\n",
       " 'embers': 725,\n",
       " 'stagner': 726,\n",
       " 'scully': 727,\n",
       " 'eulogy': 728,\n",
       " 'chans': 729,\n",
       " 'battling': 730,\n",
       " 'boards': 731,\n",
       " 'breathe': 732,\n",
       " 'borg': 733,\n",
       " 'escadapes': 734,\n",
       " 'cherry': 735,\n",
       " 'belch': 736,\n",
       " 'commodity': 737,\n",
       " 'treatise': 738,\n",
       " 'cheddar': 739,\n",
       " 'opportunistic': 740,\n",
       " 'thwart': 741,\n",
       " 'residue': 742,\n",
       " 'proffessor': 743,\n",
       " 'gratingly': 744,\n",
       " 'maybe': 745,\n",
       " 'trot': 746,\n",
       " 'roddenberry': 747,\n",
       " 'wuthering': 748,\n",
       " 'avenues': 749,\n",
       " 'shirts': 750,\n",
       " 'slinky': 751,\n",
       " 'foes': 752,\n",
       " '`dumb': 753,\n",
       " 'plently': 754,\n",
       " 'affiliated': 755,\n",
       " 'lts': 756,\n",
       " 'weirded': 757,\n",
       " 'moth': 758,\n",
       " 'midknight': 759,\n",
       " 'tights': 760,\n",
       " '`ronna': 761,\n",
       " 'youngster': 762,\n",
       " 'endeavors': 763,\n",
       " 'minds': 764,\n",
       " 'morita': 765,\n",
       " 'cared': 766,\n",
       " 'barbaric': 767,\n",
       " 'bikers': 768,\n",
       " 'dilapidated': 769,\n",
       " 'macfadyen': 770,\n",
       " 'aunts': 771,\n",
       " 'mindfuck': 772,\n",
       " 'lowlife': 773,\n",
       " 'enchanting': 774,\n",
       " 'perrier': 775,\n",
       " 'crossgates': 776,\n",
       " 'fide': 777,\n",
       " 'harped': 778,\n",
       " 'caraciture': 779,\n",
       " 'distance': 780,\n",
       " 'djimon': 781,\n",
       " 'gapes': 782,\n",
       " 'gassing': 783,\n",
       " 'rydell': 784,\n",
       " 'enlist': 785,\n",
       " 'durning': 786,\n",
       " 'improvised': 787,\n",
       " 'copacabana': 788,\n",
       " 'sufficient': 789,\n",
       " 'tourniquet': 790,\n",
       " 'highschool': 791,\n",
       " 'dumbfounded': 792,\n",
       " 'infamy': 793,\n",
       " 'gutting': 794,\n",
       " 'dial': 795,\n",
       " 'drunkard': 796,\n",
       " 'existences': 797,\n",
       " 'piling': 798,\n",
       " 'humphrey': 799,\n",
       " 'whoville': 800,\n",
       " 'pinchot': 801,\n",
       " 'export': 802,\n",
       " 'blondes': 803,\n",
       " 'loyality': 804,\n",
       " 'intimidating': 805,\n",
       " 'backside': 806,\n",
       " 'craving': 807,\n",
       " 'sorrows': 808,\n",
       " 'slapdash': 809,\n",
       " 'unspoken': 810,\n",
       " 'roomie': 811,\n",
       " 'reddish': 812,\n",
       " 'influential': 813,\n",
       " 'forgets': 814,\n",
       " 'sylvie': 815,\n",
       " 'henchlady': 816,\n",
       " 'ironically': 817,\n",
       " 'incompleteness': 818,\n",
       " 'fiddling': 819,\n",
       " 'embellished': 820,\n",
       " 'expressively': 821,\n",
       " 'wylde': 822,\n",
       " 'nosing': 823,\n",
       " 'hohh': 824,\n",
       " 'wordless': 825,\n",
       " 'inkling': 826,\n",
       " 'necroplasmic': 827,\n",
       " 'trods': 828,\n",
       " 'unavoidable': 829,\n",
       " 'nique': 830,\n",
       " 'reclusion': 831,\n",
       " 'eluding': 832,\n",
       " 'ribbing': 833,\n",
       " 'shifted': 834,\n",
       " 'saget': 835,\n",
       " 'ruining': 836,\n",
       " 'very': 837,\n",
       " 'tires': 838,\n",
       " 'directoral': 839,\n",
       " 'sirocco': 840,\n",
       " 'orca': 841,\n",
       " 'kruger': 842,\n",
       " 'looking': 843,\n",
       " 'prurient': 844,\n",
       " 'robb': 845,\n",
       " 'redman@bvoice': 846,\n",
       " 'edging': 847,\n",
       " 'beristain': 848,\n",
       " 'unraveling': 849,\n",
       " 'denton': 850,\n",
       " 'exposes': 851,\n",
       " 'outworld': 852,\n",
       " 'jailterm': 853,\n",
       " 'camp': 854,\n",
       " 'import': 855,\n",
       " 'blatantly': 856,\n",
       " 'modifier': 857,\n",
       " 'piddling': 858,\n",
       " 'loyal': 859,\n",
       " 'echoing': 860,\n",
       " 'hav': 861,\n",
       " 'writeup': 862,\n",
       " 'rhode': 863,\n",
       " 'obsess': 864,\n",
       " 'beauty': 865,\n",
       " 'ofcs': 866,\n",
       " 'expendable': 867,\n",
       " 'pain': 868,\n",
       " 'godfathers': 869,\n",
       " 'tragicomic': 870,\n",
       " 'urinals': 871,\n",
       " 'meditation': 872,\n",
       " 'lengthy': 873,\n",
       " 'madeline': 874,\n",
       " 'panel': 875,\n",
       " 'hitches': 876,\n",
       " 'hessian': 877,\n",
       " 'cheng': 878,\n",
       " 'declining': 879,\n",
       " 'churchgoers': 880,\n",
       " 'tatum': 881,\n",
       " 'swanson': 882,\n",
       " 'morlocks': 883,\n",
       " 'crimes': 884,\n",
       " 'glib': 885,\n",
       " 'genres': 886,\n",
       " 'gallup': 887,\n",
       " 'fruity': 888,\n",
       " '`ultra': 889,\n",
       " 'indulged': 890,\n",
       " 'tabloids': 891,\n",
       " 'matinee': 892,\n",
       " 'ridiculed': 893,\n",
       " 'insistent': 894,\n",
       " 'wares': 895,\n",
       " 'protests': 896,\n",
       " 'ribbons': 897,\n",
       " 'astronomer': 898,\n",
       " 'concludes': 899,\n",
       " 'centerfold': 900,\n",
       " 'indestructible': 901,\n",
       " 'hemming': 902,\n",
       " 'mercilessly': 903,\n",
       " 'opening': 904,\n",
       " 'ordering': 905,\n",
       " 'winningly': 906,\n",
       " '_disturbing_behavior_': 907,\n",
       " 'document': 908,\n",
       " 'impenetrable': 909,\n",
       " 'ozzy': 910,\n",
       " 'buffed': 911,\n",
       " 'vicious': 912,\n",
       " 'unchained': 913,\n",
       " 'teach': 914,\n",
       " 'falco': 915,\n",
       " 'brazen': 916,\n",
       " 'quintessentially': 917,\n",
       " 'beth': 918,\n",
       " 'thoroughly': 919,\n",
       " 'clapet': 920,\n",
       " 'miscasting': 921,\n",
       " 'winged': 922,\n",
       " 'althea': 923,\n",
       " 'nocturnal': 924,\n",
       " 'flipping': 925,\n",
       " 'balk': 926,\n",
       " 'godard': 927,\n",
       " 'hiave': 928,\n",
       " 'mib': 929,\n",
       " 'hos': 930,\n",
       " 'amnesia': 931,\n",
       " 'lingo': 932,\n",
       " 'outlanders': 933,\n",
       " 'requires': 934,\n",
       " '_brazil_': 935,\n",
       " 'shatner': 936,\n",
       " 'gaunt': 937,\n",
       " 'sprites': 938,\n",
       " 'philosophies': 939,\n",
       " 'gotta': 940,\n",
       " 'twentysomethings': 941,\n",
       " 'jabbering': 942,\n",
       " 'longevity': 943,\n",
       " 'stalked': 944,\n",
       " 'lawerences': 945,\n",
       " 'irma': 946,\n",
       " 'beethoven': 947,\n",
       " 'gangs': 948,\n",
       " 'patting': 949,\n",
       " 'deserts': 950,\n",
       " 'extremism': 951,\n",
       " 'ballistics': 952,\n",
       " 'airbrushed': 953,\n",
       " 'brandy': 954,\n",
       " 'butcher': 955,\n",
       " 'viola': 956,\n",
       " 'pebbles': 957,\n",
       " 'artfully': 958,\n",
       " 'levitt': 959,\n",
       " 'ayla': 960,\n",
       " 'childhood': 961,\n",
       " 'skips': 962,\n",
       " 'battlestar': 963,\n",
       " 'noticably': 964,\n",
       " 'unveils': 965,\n",
       " 'links': 966,\n",
       " 'castles': 967,\n",
       " 'excavating': 968,\n",
       " 'quickie': 969,\n",
       " 'ravishingly': 970,\n",
       " 'breeds': 971,\n",
       " 'actionless': 972,\n",
       " 'au': 973,\n",
       " 'fray': 974,\n",
       " 'compendium': 975,\n",
       " 'reproduced': 976,\n",
       " '?': 977,\n",
       " 'assignments': 978,\n",
       " 'tissues': 979,\n",
       " 'shockingly': 980,\n",
       " 'accommodating': 981,\n",
       " 'signifiers': 982,\n",
       " 'jurgen': 983,\n",
       " 'earthling': 984,\n",
       " 'vining': 985,\n",
       " 'anya': 986,\n",
       " 'flabbergasted': 987,\n",
       " 'brent': 988,\n",
       " 'autobiographical': 989,\n",
       " 'guru': 990,\n",
       " 'riiiiight': 991,\n",
       " 'pickers': 992,\n",
       " 'glassy': 993,\n",
       " 'needing': 994,\n",
       " 'pep': 995,\n",
       " 'walken': 996,\n",
       " 'strand': 997,\n",
       " 'chill': 998,\n",
       " 'exude': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 39682)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Expliquer comment les classes positives et négatives ont été assignées sur les critiques de films"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section describes how we determined whether a review was positive\n",
    "or negative.\n",
    "\n",
    "The original html files do not have consistent formats -- a review may\n",
    "not have the author's rating with it, and when it does, the rating can\n",
    "appear at different places in the file in different forms.  We only\n",
    "recognize some of the more explicit ratings, which are extracted via a\n",
    "set of ad-hoc rules.  In essence, a file's classification is determined\n",
    "based on the first rating we were able to identify.\n",
    "\n",
    "\n",
    "- In order to obtain more accurate rating decisions, the maximum\n",
    "\trating must be specified explicitly, both for numerical ratings\n",
    "\tand star ratings.  (\"8/10\", \"four out of five\", and \"OUT OF\n",
    "\t****: ***\" are examples of rating indications we recognize.)\n",
    "\n",
    "- With a five-star system (or compatible number systems):\n",
    "\tthree-and-a-half stars and up are considered positive, \n",
    "\ttwo stars and below are considered negative.\n",
    "- With a four-star system (or compatible number system):\n",
    "\tthree stars and up are considered positive, \n",
    "\tone-and-a-half stars and below are considered negative.  \n",
    "- With a letter grade system:\n",
    "\tB or above is considered positive,\n",
    "\tC- or below is considered negative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 3) Compléter la classe NB pour qu’elle implémente le classifieur Naive Bayes en vous appuyant sur le pseudo-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'ai fait le choix d'inclure la création de matrix count per document dans mon classifier. Il prendra en input le corpus de textes et appellera la méthode count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NB(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self):\n",
    "        self.prior = None\n",
    "        self.condprob = None\n",
    "        self.vocab = None\n",
    "        self.matrix = None\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        #effectuer le wordcount\n",
    "        self.vocab, self.matrix = count_words(X)\n",
    "        \n",
    "        y_0 = y[y==0]\n",
    "        y_1 = y[y==1]\n",
    "        matrix_0 = self.matrix[y==0]\n",
    "        matrix_1 = self.matrix[y==1]\n",
    "        \n",
    "        #compute prior probability : P(y=k)\n",
    "        prior_y_0 = len(y_0)/len(y)\n",
    "        prior_y_1 = 1 -  prior_y_0\n",
    "        \n",
    "        #Class y=0\n",
    "        cond_prob_0 = np.zeros(self.matrix.shape[1]) #1 vector of probabilities for all the words\n",
    "        #somme du nombre d'occurences du mot +1\n",
    "        sum_on_word_0 = matrix_0.sum(axis=0) + 1\n",
    "        #somme de la somme du nombre d'occurences sur chaque mot +1\n",
    "        total_sum_on_0 = (matrix_0.sum(axis=0) + 1).sum()\n",
    "        #P(mot|Y=0)\n",
    "        cond_prob_0 = sum_on_word_0 / total_sum_on_0\n",
    "        \n",
    "        #Class y=1\n",
    "        cond_prob_1 = np.zeros(self.matrix.shape[1]) #1 vector of probabilities for all the words\n",
    "        #somme du nombre d'occurences du mot +1\n",
    "        sum_on_word_1 = matrix_1.sum(axis=0) + 1\n",
    "        #somme de la somme du nombre d'occurences sur chaque mot +1\n",
    "        total_sum_on_1 = (matrix_1.sum(axis=0) + 1).sum()\n",
    "        #P(mot|Y=1)\n",
    "        cond_prob_1 = sum_on_word_1 / total_sum_on_1\n",
    "        \n",
    "        self.prior = np.array((prior_y_0,prior_y_1))\n",
    "        self.condprob = np.vstack((cond_prob_0,cond_prob_1))\n",
    "        return self\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        vocab_test,matrix_test = count_words(X)\n",
    "        #gardons uniquement les mots en commun entre le test et le train\n",
    "        #recuperons les mots de chacun dans un set et gardons l'intersection des mots\n",
    "        keys_train = self.vocab.keys()\n",
    "        keys_test = vocab_test.keys()\n",
    "        keys_intersect = keys_train & keys_test\n",
    "        \n",
    "        #recuperons les indices associées pour le train & test dans des listes\n",
    "        idx_keys_train = []\n",
    "        idx_keys_test = []\n",
    "        for key in keys_intersect:\n",
    "            idx_keys_train.append(self.vocab[key])\n",
    "            idx_keys_test.append(vocab_test[key])\n",
    "        \n",
    "        #renvoyer les cond_probabilities et le count du test associé aux indices\n",
    "        condprob_new = self.condprob[:,idx_keys_train]\n",
    "        matrix_test = matrix_test[:,idx_keys_test]\n",
    "        \n",
    "        #classe y = 0\n",
    "        #J'ajoute la prior proba\n",
    "        score_0 = np.zeros(matrix_test.shape[0])\n",
    "        score_0+=np.log(self.prior[0])\n",
    "        \n",
    "        #add cond_prob*occurence\n",
    "        prod = np.dot(matrix_test,np.log(condprob_new[0]))\n",
    "        score_0+=prod\n",
    "        \n",
    "        \n",
    "        #classe y = 1\n",
    "        #J'ajoute la prior proba\n",
    "        score_1 = np.zeros(matrix_test.shape[0])\n",
    "        score_1+=np.log(self.prior[1])\n",
    "        \n",
    "        #cond_prob matrix\n",
    "        #prod = np.dot(matrix_test,np.log(condprob_new[1]))\n",
    "        prod = np.dot(matrix_test,np.log(condprob_new[1]))\n",
    "        score_1+=prod\n",
    "        \n",
    "        y_predict = np.zeros(matrix_test.shape[0])\n",
    "        y_predict[score_1>score_0] = 1\n",
    "        return y_predict\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return np.mean(self.predict(X) == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NB()"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = NB()\n",
    "nb.fit(texts,y)\n",
    "#train error \n",
    "#nb.score(texts,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Evaluer les performances de votre classifieur en cross-validation 5-folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.805   0.8225  0.8075  0.8275  0.7875] moyenne :  0.81\n"
     ]
    }
   ],
   "source": [
    "#METHOD 2 WITH cross_val_score()\n",
    "res = cross_val_score(nb,texts,y,cv=5)\n",
    "print(res, \"moyenne : \", res.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) Modifiez la fonction count_words pour qu’elle ignore les “stop words” dans le fichier data/english.stop. Les performances sont-elles améliorées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read stop word file in a list\n",
    "with open('/Users/mehdiregina/Documents/jupyter/MDI343/TPTextMining/data/english.stop','r') as text_file:\n",
    "    stop_words = text_file.read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " \"a's\",\n",
       " 'able',\n",
       " 'about',\n",
       " 'above',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'across',\n",
       " 'actually',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " \"ain't\",\n",
       " 'all',\n",
       " 'allow',\n",
       " 'allows',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anyways',\n",
       " 'anywhere',\n",
       " 'apart',\n",
       " 'appear',\n",
       " 'appreciate',\n",
       " 'appropriate',\n",
       " 'are',\n",
       " \"aren't\",\n",
       " 'around',\n",
       " 'as',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asking',\n",
       " 'associated',\n",
       " 'at',\n",
       " 'available',\n",
       " 'away',\n",
       " 'awfully',\n",
       " 'b',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'believe',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'best',\n",
       " 'better',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'both',\n",
       " 'brief',\n",
       " 'but',\n",
       " 'by',\n",
       " 'c',\n",
       " \"c'mon\",\n",
       " \"c's\",\n",
       " 'came',\n",
       " 'can',\n",
       " \"can't\",\n",
       " 'cannot',\n",
       " 'cant',\n",
       " 'cause',\n",
       " 'causes',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'changes',\n",
       " 'clearly',\n",
       " 'co',\n",
       " 'com',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'concerning',\n",
       " 'consequently',\n",
       " 'consider',\n",
       " 'considering',\n",
       " 'contain',\n",
       " 'containing',\n",
       " 'contains',\n",
       " 'corresponding',\n",
       " 'could',\n",
       " \"couldn't\",\n",
       " 'course',\n",
       " 'currently',\n",
       " 'd',\n",
       " 'definitely',\n",
       " 'described',\n",
       " 'despite',\n",
       " 'did',\n",
       " \"didn't\",\n",
       " 'different',\n",
       " 'do',\n",
       " 'does',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " \"don't\",\n",
       " 'done',\n",
       " 'down',\n",
       " 'downwards',\n",
       " 'during',\n",
       " 'e',\n",
       " 'each',\n",
       " 'edu',\n",
       " 'eg',\n",
       " 'eight',\n",
       " 'either',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'enough',\n",
       " 'entirely',\n",
       " 'especially',\n",
       " 'et',\n",
       " 'etc',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everybody',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'ex',\n",
       " 'exactly',\n",
       " 'example',\n",
       " 'except',\n",
       " 'f',\n",
       " 'far',\n",
       " 'few',\n",
       " 'fifth',\n",
       " 'first',\n",
       " 'five',\n",
       " 'followed',\n",
       " 'following',\n",
       " 'follows',\n",
       " 'for',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forth',\n",
       " 'four',\n",
       " 'from',\n",
       " 'further',\n",
       " 'furthermore',\n",
       " 'g',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'getting',\n",
       " 'given',\n",
       " 'gives',\n",
       " 'go',\n",
       " 'goes',\n",
       " 'going',\n",
       " 'gone',\n",
       " 'got',\n",
       " 'gotten',\n",
       " 'greetings',\n",
       " 'h',\n",
       " 'had',\n",
       " \"hadn't\",\n",
       " 'happens',\n",
       " 'hardly',\n",
       " 'has',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he's\",\n",
       " 'hello',\n",
       " 'help',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " \"here's\",\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " 'hereupon',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'hi',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'hither',\n",
       " 'hopefully',\n",
       " 'how',\n",
       " 'howbeit',\n",
       " 'however',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " 'ie',\n",
       " 'if',\n",
       " 'ignored',\n",
       " 'immediate',\n",
       " 'in',\n",
       " 'inasmuch',\n",
       " 'inc',\n",
       " 'indeed',\n",
       " 'indicate',\n",
       " 'indicated',\n",
       " 'indicates',\n",
       " 'inner',\n",
       " 'insofar',\n",
       " 'instead',\n",
       " 'into',\n",
       " 'inward',\n",
       " 'is',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'j',\n",
       " 'just',\n",
       " 'k',\n",
       " 'keep',\n",
       " 'keeps',\n",
       " 'kept',\n",
       " 'know',\n",
       " 'knows',\n",
       " 'known',\n",
       " 'l',\n",
       " 'last',\n",
       " 'lately',\n",
       " 'later',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'lest',\n",
       " 'let',\n",
       " \"let's\",\n",
       " 'like',\n",
       " 'liked',\n",
       " 'likely',\n",
       " 'little',\n",
       " 'look',\n",
       " 'looking',\n",
       " 'looks',\n",
       " 'ltd',\n",
       " 'm',\n",
       " 'mainly',\n",
       " 'many',\n",
       " 'may',\n",
       " 'maybe',\n",
       " 'me',\n",
       " 'mean',\n",
       " 'meanwhile',\n",
       " 'merely',\n",
       " 'might',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'much',\n",
       " 'must',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'n',\n",
       " 'name',\n",
       " 'namely',\n",
       " 'nd',\n",
       " 'near',\n",
       " 'nearly',\n",
       " 'necessary',\n",
       " 'need',\n",
       " 'needs',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'new',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'non',\n",
       " 'none',\n",
       " 'noone',\n",
       " 'nor',\n",
       " 'normally',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'novel',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'o',\n",
       " 'obviously',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'oh',\n",
       " 'ok',\n",
       " 'okay',\n",
       " 'old',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'ones',\n",
       " 'only',\n",
       " 'onto',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'ought',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'outside',\n",
       " 'over',\n",
       " 'overall',\n",
       " 'own',\n",
       " 'p',\n",
       " 'particular',\n",
       " 'particularly',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'placed',\n",
       " 'please',\n",
       " 'plus',\n",
       " 'possible',\n",
       " 'presumably',\n",
       " 'probably',\n",
       " 'provides',\n",
       " 'q',\n",
       " 'que',\n",
       " 'quite',\n",
       " 'qv',\n",
       " 'r',\n",
       " 'rather',\n",
       " 'rd',\n",
       " 're',\n",
       " 'really',\n",
       " 'reasonably',\n",
       " 'regarding',\n",
       " 'regardless',\n",
       " 'regards',\n",
       " 'relatively',\n",
       " 'respectively',\n",
       " 'right',\n",
       " 's',\n",
       " 'said',\n",
       " 'same',\n",
       " 'saw',\n",
       " 'say',\n",
       " 'saying',\n",
       " 'says',\n",
       " 'second',\n",
       " 'secondly',\n",
       " 'see',\n",
       " 'seeing',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'seen',\n",
       " 'self',\n",
       " 'selves',\n",
       " 'sensible',\n",
       " 'sent',\n",
       " 'serious',\n",
       " 'seriously',\n",
       " 'seven',\n",
       " 'several',\n",
       " 'shall',\n",
       " 'she',\n",
       " 'should',\n",
       " \"shouldn't\",\n",
       " 'since',\n",
       " 'six',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somebody',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhat',\n",
       " 'somewhere',\n",
       " 'soon',\n",
       " 'sorry',\n",
       " 'specified',\n",
       " 'specify',\n",
       " 'specifying',\n",
       " 'still',\n",
       " 'sub',\n",
       " 'such',\n",
       " 'sup',\n",
       " 'sure',\n",
       " 't',\n",
       " \"t's\",\n",
       " 'take',\n",
       " 'taken',\n",
       " 'tell',\n",
       " 'tends',\n",
       " 'th',\n",
       " 'than',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'thanx',\n",
       " 'that',\n",
       " \"that's\",\n",
       " 'thats',\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " \"there's\",\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " 'therefore',\n",
       " 'therein',\n",
       " 'theres',\n",
       " 'thereupon',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'think',\n",
       " 'third',\n",
       " 'this',\n",
       " 'thorough',\n",
       " 'thoroughly',\n",
       " 'those',\n",
       " 'though',\n",
       " 'three',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'to',\n",
       " 'together',\n",
       " 'too',\n",
       " 'took',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'tried',\n",
       " 'tries',\n",
       " 'truly',\n",
       " 'try',\n",
       " 'trying',\n",
       " 'twice',\n",
       " 'two',\n",
       " 'u',\n",
       " 'un',\n",
       " 'under',\n",
       " 'unfortunately',\n",
       " 'unless',\n",
       " 'unlikely',\n",
       " 'until',\n",
       " 'unto',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'use',\n",
       " 'used',\n",
       " 'useful',\n",
       " 'uses',\n",
       " 'using',\n",
       " 'usually',\n",
       " 'uucp',\n",
       " 'v',\n",
       " 'value',\n",
       " 'various',\n",
       " 'very',\n",
       " 'via',\n",
       " 'viz',\n",
       " 'vs',\n",
       " 'w',\n",
       " 'want',\n",
       " 'wants',\n",
       " 'was',\n",
       " \"wasn't\",\n",
       " 'way',\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " \"we've\",\n",
       " 'welcome',\n",
       " 'well',\n",
       " 'went',\n",
       " 'were',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " \"what's\",\n",
       " 'whatever',\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'where',\n",
       " \"where's\",\n",
       " 'whereafter',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'wherein',\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'while',\n",
       " 'whither',\n",
       " 'who',\n",
       " \"who's\",\n",
       " 'whoever',\n",
       " 'whole',\n",
       " 'whom',\n",
       " 'whose',\n",
       " 'why',\n",
       " 'will',\n",
       " 'willing',\n",
       " 'wish',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " \"won't\",\n",
       " 'wonder',\n",
       " 'would',\n",
       " 'would',\n",
       " \"wouldn't\",\n",
       " 'x',\n",
       " 'y',\n",
       " 'yes',\n",
       " 'yet',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'z',\n",
       " 'zero']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check de la liste des stops words donnée\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_new_text (texts, stop_words):\n",
    "    \"\"\"Génère un nouveau corpus de texte sans les stops words\"\"\"\n",
    "    set_stop_words = set(stop_words)\n",
    "    new_texts = []\n",
    "    for text in texts:\n",
    "        word_text = []\n",
    "        for word in text.split():\n",
    "            if word not in stop_words:\n",
    "                word_text.append(word)\n",
    "        new_texts.append(\" \".join(word_text))\n",
    "    return new_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_texts = gen_new_text(texts,stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.795  0.815  0.8    0.83   0.765] moyenne :  0.801\n"
     ]
    }
   ],
   "source": [
    "nb.fit(new_texts,y)\n",
    "res = cross_val_score(nb,new_texts,y,cv=5)\n",
    "print(res, \"moyenne : \", res.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe pas d'amélioration après exclusion de la liste des stop words. Il se peut que certains mots de cette liste étaient des features utiles à l'explication de la classification des textes comme 'awfully','appreciate', tandis que d'autres étaient effectivement négligeables. Au global cette liste de stop_word n'améliore pas l'accuracy du modèle, il serait néanmoins intéressant de la retravailler pour l'adapter à notre cas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Utilisation de sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk import SnowballStemmer, pos_tag, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Comparer votre implémentation avec scikitlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Utilisation d'une pipeline\n",
    "pipeline_nb = Pipeline([('count_vec',CountVectorizer()),('nb_skl',MultinomialNB())])\n",
    "#essai uniquement avec unigram et bigram à cause du cout computationnel\n",
    "parameters = {'count_vec__analyzer':['word','char', 'char_wb'],'count_vec__stop_words':[None,'english'],\n",
    "             'count_vec__ngram_range': [(1, 1),(1, 2)]} \n",
    "grid_cv = GridSearchCV(pipeline_nb,param_grid=parameters,cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('count_vec', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)), ('nb_skl', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'count_vec__analyzer': ['word', 'char', 'char_wb'], 'count_vec__stop_words': [None, 'english'], 'count_vec__ngram_range': [(1, 1), (1, 2)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv.fit(texts,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83050000000000002"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count_vec__analyzer': 'word',\n",
       " 'count_vec__ngram_range': (1, 2),\n",
       " 'count_vec__stop_words': None}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A l'issue du grid search on constate qu'on obtient une meilleure accuracy que l'estimateur maison en travaillant avec des unigrams et bi-grams (1,2) de mots (analyzer = word) et en ne prenant pas en compte de liste de stop words. On pourrait imaginer obtenir encore de meilleurs scores en testant des n-grams plus grand, je ne l'ai néanmoins pas fait pour des raisons computationnelles.\n",
    "L'avantage du n-grams (à partir de 2) est qu'il permet de capturer un sens contrairement à l'unigram. On va en effet tenir compte des n-1 mots précédent le mot observé. On pourrait donc penser intuitivement que plus le n-gram est grand mieux on va capturer le sens jusqu'à une certaine limite ou on ne pourra plus généraliser notre modèle car il sera devenu trop spécifique à notre train set.\n",
    "\n",
    "\n",
    "A noter que si on considère les mêmes paramètres que l'estimateur maison, c'est à dire avec des unigrams de mots, on obtient une accuracy similaire néanmoins l'exécution est plus rapide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Tester un autre algorithme de la librairie scikitlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'count_vec__analyzer': 'word',\n",
       "   'count_vec__ngram_range': (1, 2),\n",
       "   'count_vec__stop_words': None},\n",
       "  0.84999999999999998),\n",
       " ({'count_vec__analyzer': 'word',\n",
       "   'count_vec__ngram_range': (1, 2),\n",
       "   'count_vec__stop_words': None},\n",
       "  0.85250000000000004)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liste_pipeline = []\n",
    "liste_pipeline.append(Pipeline([('count_vec',CountVectorizer()),('linear_svc',LinearSVC())]))\n",
    "liste_pipeline.append(Pipeline([('count_vec',CountVectorizer()),('log_reg',LogisticRegression())]))\n",
    "\n",
    "liste_score = []\n",
    "liste_param = []\n",
    "res = []\n",
    "\n",
    "for pip in liste_pipeline:\n",
    "    grid = GridSearchCV(pip,param_grid=parameters,cv=5)\n",
    "    grid.fit(texts,y)\n",
    "    liste_score.append(grid.best_score_)\n",
    "    liste_param.append(grid.best_params_)\n",
    "    res = list(zip(liste_param,liste_score))\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate que le Naive Bayes a une performance (accuracy) inférieure aux Linar SVC(0.850) et à la regression logistique(0.853). Cela peut être causé par l'hypothèse forte du model génératif Naive Bayes à savoir que P(X|y=k) est égale aux produits des probabilités marginales de chaque feautre, les features sont donc considérées indépendantes. Nous savons que l'apparition d'un mot est dépendante de celle des mots qui le précède, on peut donc se demander si les features d'occurences des mots sont toutes indépendantes entre elles.\n",
    "\n",
    "Les classes sont parfaitement équilibrées par conséquent l'utilisation de la regression logistique est appropriée. De plus la méthode de sklearn comporte par défaut une pénalisation l2 qui permet à l'algo de rester stable malgré la grande dimension. Il serait intéressant d'observer les résultats avec une pénalisation parcimonieuse comme l1. Le bon score de la logistic regression montre que l'espace des critiques positives et négatives est séparable par un hyperplan.\n",
    "\n",
    "En outre le SVM reste stable même si le dataset est en grande dimension. Sachant qu'un classifier linéaire (log.reg) a une bonne accuracy, on peut se limiter au linearSVC qui sera moins gourmand en temps de calculs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Utiliser la librairie NLTK afin de procéder à une racinisation (stemming)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arabic danish dutch english finnish french german hungarian italian norwegian porter portuguese romanian russian spanish swedish\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(SnowballStemmer.languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Racinisation de notre corpus de text\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "texts_stemmed = []\n",
    "for text in texts:\n",
    "    words_list = re.split(r'[\\n\\s\\.,-;\\':]+',text.strip())\n",
    "    words_list_stem = [stemmer.stem(word) for word in words_list]\n",
    "    texts_stemmed.append(\" \".join(words_list_stem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plot two teen coupl go to a church parti drink and then drive they get into an accid one of the guy die but his girlfriend continu to see him in her life and has nightmar what s the deal ? watch the movi and \" sorta \" find out critiqu a mind fuck movi for the teen generat that touch on a veri cool idea but present it in a veri bad packag which is what make this review an even harder one to write sinc i general applaud film which attempt to break the mold mess with your head and such ( lost highway & memento ) but there are good and bad way of make all type of film and these folk just didn t snag this one correct they seem to have taken this pretti neat concept but execut it terribl so what are the problem with the movi ? well it main problem is that it s simpli too jumbl it start off \" normal \" but then downshift into this \" fantasi \" world in which you as an audienc member have no idea what s go on there are dream there are charact come back from the dead there are other who look like the dead there are strang apparit there are disappear there are a looooot of chase scene there are ton of weird thing that happen and most of it is simpli not explain now i person don t mind tri to unravel a film everi now and then but when all it doe is give me the same clue over and over again i get kind of fed up after a while which is this film s biggest problem it s obvious got this big secret to hide but it seem to want to hide it complet until it final five minut and do they make thing entertain thrill or even engag in the meantim ? not realli the sad part is that the arrow and i both dig on flick like this so we actual figur most of it out by the half way point so all of the strang after that did start to make a littl bit of sens but it still didn t the make the film all that more entertain i guess the bottom line with movi like this is that you should alway make sure that the audienc is \" into it \" even befor they are given the secret password to enter your world of understand i mean show melissa sagemil run away from vision for about minut throughout the movi is just plain lazi ! ! okay we get it there are peopl chase her and we don t know who they are do we realli need to see it over and over again ? how about give us differ scene offer further insight into all of the strang go down in the movi ? appar the studio took this film away from it director and chop it up themselv and it show there might ve been a pretti decent teen mind fuck movi in here somewher but i guess \" the suit \" decid that turn it into a music video with littl edg would make more sens the actor are pretti good for the most part although wes bentley just seem to be play the exact same charact that he did in american beauti onli in a new neighborhood but my biggest kudo go out to sagemil who hold her own throughout the entir film and actual has you feel her charact s unravel overal the film doesn t stick becaus it doesn t entertain it s confus it rare excit and it feel pretti redund for most of it runtim despit a pretti cool end and explan to all of the crazi that came befor it oh and by the way this is not a horror or teen slasher flick it s just packag to look that way becaus someon is appar assum that the genr is still hot with the kid it also wrap product two year ago and has been sit on the shelv ever sinc whatev skip it ! where s joblo come from ? a nightmar of elm street ( ) blair witch ( ) the crow ( ) the crow salvat ( ) lost highway ( ) memento ( ) the other ( ) stir of echo ( )'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verification\n",
    "texts_stemmed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stem_word(text):\n",
    "    \"\"\"Retourne pour un texte donné une liste de mots splittés et stemmés\"\"\"\n",
    "    words_list = re.split(r'[\\n\\s\\.,-;\\':]+',text.strip())\n",
    "    return [stemmer.stem(word) for word in words_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline_log = Pipeline([('count_vec',CountVectorizer(analyzer='word',tokenizer=stem_word,ngram_range=(1,2))),\n",
    "                         ('log_reg',LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.8225  0.8325  0.82    0.8475  0.855 ] moyenne :  0.8355\n"
     ]
    }
   ],
   "source": [
    "accuracy_score = cross_val_score(pipeline_log,texts,y,cv=5)\n",
    "print(accuracy_score,\"moyenne : \",accuracy_score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pas d'amélioration notable de l'accuracy à l'issu de la racinisation, elle permet une réduction dimensionnelle ce n'est néanmoins pas payant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Filtrer les mots par catégorie grammaticale (POS : Part Of Speech) et ne garder que les noms, les verbes, les adverbes et les adjectifs pour la classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mehdiregina/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "texts_trie = []\n",
    "for text in texts :\n",
    "    text = word_tokenize(text) #etape prealable necessaire\n",
    "    buffer = pos_tag(text)\n",
    "    #je conserve uniquement adjectif(JJ), nom (NN), verbe (VBP), adverbes(RB)\n",
    "    liste_trie = [tup[0] for tup in buffer if tup[1] in ['JJ','NN','VBP','RB']]\n",
    "    texts_trie.append(\" \".join(liste_trie))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"plot teen go church party drink then drive get accident guys girlfriend life deal movie sorta critique mind-fuck movie teen generation very cool idea very bad package review even i generally applaud attempt mold mess head such highway memento are good bad just n't correctly seem pretty neat concept terribly are movie well main problem simply too normal then fantasy world audience member have idea are are back dead are look dead are strange are are looooot chase are weird happen simply not now personally do n't film now then same clue over again i get kind while film problem obviously big secret completely final do even meantime not really sad part arrow dig actually half-way point strangeness little bit sense still n't film entertaining i bottom line always sure audience even are secret password world i mean melissa sagemiller away movie just plain lazy okay get are do n't are do really over again different further insight strangeness movie apparently studio film away director 've pretty decent teen mind-fuck movie here somewhere guess music video little edge sense are pretty good part wes bentley just exact same character american beauty only new neighborhood kudos go own entire film actually character unraveling overall film n't n't rarely pretty redundant runtime pretty cool explanation craziness way not horror teen flick just way someone apparently genre still hot also production ago ever whatever joblo nightmare elm street blair witch crow crow salvation highway memento stir\""
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#VERIFICATION\n",
    "texts_trie[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tag_tokenizer(text):\n",
    "    \"\"\"Retourne pour un texte donné une liste de mots splittés et triés\"\"\"\n",
    "    text = word_tokenize(text)\n",
    "    buffer = pos_tag(text)\n",
    "    return [tup[0] for tup in buffer if tup[1] in ['JJ','NN','VBP','RB']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.83    0.8275  0.8275  0.8425  0.8525] moyenne :  0.836\n"
     ]
    }
   ],
   "source": [
    "pipeline_log = Pipeline([('count_vec',CountVectorizer(analyzer='word',tokenizer=tag_tokenizer,ngram_range=(1,2))),\n",
    "                         ('log_reg',LogisticRegression())])\n",
    "\n",
    "accuracy_score = cross_val_score(pipeline_log,texts,y,cv=5)\n",
    "print(accuracy_score,\"moyenne : \",accuracy_score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stem_tag_tokenizer(text):\n",
    "    \"\"\"Retourne pour un texte donné une liste de mots splittés triés puis stemmé\"\"\"\n",
    "    liste_trie = tag_tokenizer(text)\n",
    "    return [stemmer.stem(word) for word in liste_trie]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.8225  0.8325  0.82    0.8475  0.855 ] moyenne :  0.8355\n"
     ]
    }
   ],
   "source": [
    "pipeline_log = Pipeline([('count_vec',CountVectorizer(analyzer='word',tokenizer=stem_tag_tokenizer,ngram_range=(1,2))),\n",
    "                         ('log_reg',LogisticRegression())])\n",
    "\n",
    "accuracy_score = cross_val_score(pipeline_log,texts,y,cv=5)\n",
    "print(accuracy_score,\"moyenne : \",accuracy_score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De même un tri suivit d'une racinisation entraine une réduction dimensionnelle non négligeable de notre dataset, néanmoins l'accuracy de notre modèle n'est pas améliorée."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
